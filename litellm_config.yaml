# LiteLLM Proxy Configuration for Mermaid
# This file configures all available LLM models

model_list:
  # ==========================================
  # Local Models via Ollama
  # ==========================================

  # Qwen models - Available on system
  - model_name: ollama/qwen3-coder:30b
    litellm_params:
      model: ollama/qwen3-coder:30b
      api_base: http://10.88.0.1:11434  # Podman gateway IP
      stream: true

  # Tiny models (< 2B params) - Fast, good for testing
  - model_name: ollama/tinyllama
    litellm_params:
      model: ollama/tinyllama
      api_base: http://10.88.0.1:11434
      stream: true

  - model_name: ollama/phi
    litellm_params:
      model: ollama/phi
      api_base: http://10.88.0.1:11434
      stream: true

  # Small models (7B params) - Good balance
  - model_name: ollama/llama2
    litellm_params:
      model: ollama/llama2
      api_base: http://10.88.0.1:11434
      stream: true

  - model_name: ollama/mistral
    litellm_params:
      model: ollama/mistral
      api_base: http://10.88.0.1:11434
      stream: true

  - model_name: ollama/codellama
    litellm_params:
      model: ollama/codellama
      api_base: http://10.88.0.1:11434
      stream: true

  # Medium models (13B params)
  - model_name: ollama/codellama:13b
    litellm_params:
      model: ollama/codellama:13b
      api_base: http://10.88.0.1:11434
      stream: true

  # Large models (33B+ params) - Best quality
  - model_name: ollama/deepseek-coder:33b
    litellm_params:
      model: ollama/deepseek-coder:33b
      api_base: http://10.88.0.1:11434
      stream: true

  - model_name: ollama/llama2:70b
    litellm_params:
      model: ollama/llama2:70b
      api_base: http://10.88.0.1:11434
      stream: true

  # ==========================================
  # OpenAI Models
  # ==========================================

  - model_name: openai/gpt-4o
    litellm_params:
      model: gpt-4o
      api_key: os.environ/OPENAI_API_KEY
      temperature: 0.7
      max_tokens: 4096

  - model_name: openai/gpt-4-turbo
    litellm_params:
      model: gpt-4-turbo-preview
      api_key: os.environ/OPENAI_API_KEY

  - model_name: openai/gpt-3.5-turbo
    litellm_params:
      model: gpt-3.5-turbo
      api_key: os.environ/OPENAI_API_KEY

  # ==========================================
  # Anthropic Claude Models
  # ==========================================

  - model_name: anthropic/claude-3-opus
    litellm_params:
      model: claude-3-opus-20240229
      api_key: os.environ/ANTHROPIC_API_KEY
      max_tokens: 4096

  - model_name: anthropic/claude-3-sonnet
    litellm_params:
      model: claude-3-5-sonnet-20241022
      api_key: os.environ/ANTHROPIC_API_KEY
      max_tokens: 4096

  - model_name: anthropic/claude-3-haiku
    litellm_params:
      model: claude-3-haiku-20240307
      api_key: os.environ/ANTHROPIC_API_KEY
      max_tokens: 4096

  # ==========================================
  # Groq (Fast Inference)
  # ==========================================

  - model_name: groq/llama3-70b
    litellm_params:
      model: llama3-70b-8192
      api_key: os.environ/GROQ_API_KEY

  - model_name: groq/mixtral-8x7b
    litellm_params:
      model: mixtral-8x7b-32768
      api_key: os.environ/GROQ_API_KEY

  # ==========================================
  # Google Models
  # ==========================================

  - model_name: google/gemini-pro
    litellm_params:
      model: gemini-pro
      api_key: os.environ/GOOGLE_API_KEY

  - model_name: google/gemini-pro-vision
    litellm_params:
      model: gemini-pro-vision
      api_key: os.environ/GOOGLE_API_KEY

  # ==========================================
  # Azure OpenAI (if configured)
  # ==========================================

  # Uncomment and configure if using Azure
  # - model_name: azure/gpt-4
  #   litellm_params:
  #     model: azure/my-gpt4-deployment
  #     api_base: os.environ/AZURE_API_BASE
  #     api_key: os.environ/AZURE_API_KEY
  #     api_version: "2024-02-15-preview"

# ==========================================
# General Settings
# ==========================================

general_settings:
  # Master key for admin access to proxy
  master_key: os.environ/LITELLM_MASTER_KEY

  # Database for persistent storage
  database_url: os.environ/DATABASE_URL

  # Custom auth support (commented out - causes error with boolean value)
  # custom_auth: false

  # Max parallel requests
  max_parallel_requests: 100

  # Inactivity timeout (seconds)
  inactivity_timeout: 600

# ==========================================
# Router Settings (Load Balancing & Retries)
# ==========================================

router_settings:
  routing_strategy: usage-based-routing  # or simple-shuffle, least-busy

  # Model fallbacks (if primary fails, try these)
  model_fallbacks:
    "openai/gpt-4o": ["openai/gpt-4-turbo", "anthropic/claude-3-sonnet"]
    "anthropic/claude-3-opus": ["openai/gpt-4o", "anthropic/claude-3-sonnet"]

  # Retry configuration
  num_retries: 2
  retry_after: 5  # seconds

  # Request timeout
  timeout: 600  # seconds (10 minutes for long responses)

  # Cache settings (optional)
  # cache: true
  # cache_ttl: 3600  # 1 hour

# ==========================================
# LiteLLM Settings
# ==========================================

litellm_settings:
  # Drop unsupported params instead of failing
  drop_params: true

  # Set verbosity
  set_verbose: false

  # JSON logs
  json_logs: false

  # Success callback
  success_callback: ["prometheus"]  # Add langfuse, etc. if needed

  # Max tokens default
  max_tokens: 4096

  # Temperature default
  temperature: 0.7